# ===== s3a/minio =====
spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.endpoint=http://localhost:9000  
# spark.hadoop.fs.s3a.endpoint=http://minio:9000    # docker
spark.hadoop.fs.s3a.region=ap-northeast-2           # 서울
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true

# ===== delta lake ===== 
spark.databricks.delta.retentionDurationCheck.enabled=false
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# ===== spark =====
#spark.master=local[*]                              # 로컬 환경
spark.master=spark://localhost:7077
# spark.master=spark://spark-master:7077           # docker

# ===== eventlog  =====
spark.eventlog.enabled=true
spark.eventlog.dir=file:///tmp/spark-events         # local
spark.history.fs.logDirectory=file:///tmp/spark-events  # 대소문자 수정

# ===== packages ===== (Spark 4.0.0 + Delta Lake 4.0.0 Preview)
# 주의: delta-core에서 delta-spark로 패키지명 변경됨
spark.jars.packages=io.delta:delta-spark_2.13:4.0.0,org.apache.hadoop:hadoop-aws:3.4.0,org.postgresql:postgresql:42.7.3
spark.jars.packages=io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-aws:3.3.2,org.postgresql:postgresql:42.7.3

# ===== 추가 Delta Lake 설정 =====
spark.sql.catalog.spark_catalog.type=delta
spark.databricks.delta.merge.repartitionBeforeWrite.enabled=true
spark.databricks.delta.schema.autoMerge.enabled=true

# ===== metrics =====
spark.sql.codegen.wholeStage=false
spark.ui.prometheus.enabled=true

# ===== 추가 S3A 최적화 설정 =====
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.block.size=128M
spark.hadoop.fs.s3a.multipart.size=100M
spark.hadoop.fs.s3a.multipart.threshold=128M
spark.hadoop.fs.s3a.committer.name=magic
spark.hadoop.fs.s3a.committer.magic.enabled=true
